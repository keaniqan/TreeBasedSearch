{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6de3ba25",
   "metadata": {},
   "source": [
    "Importing the required libraries and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "334537ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pathlib, json, numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "SEED = 42\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH = 32\n",
    "\n",
    "ROOT_DIR = pathlib.Path(\"data/\")\n",
    "TRAIN_DIR = ROOT_DIR / \"training\"\n",
    "VAL_DIR   = ROOT_DIR / \"validation\"\n",
    "MODEL_DIR = \"models\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd156a7",
   "metadata": {},
   "source": [
    "This file is for training the various image classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2f7a647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1364 files belonging to 3 classes.\n",
      "Found 248 files belonging to 3 classes.\n",
      "Classes: ['01-minor', '02-moderate', '03-severe']\n"
     ]
    }
   ],
   "source": [
    "import pathlib, tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Build datasets WITHOUT validation_split\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    seed=SEED,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    VAL_DIR,\n",
    "    seed=SEED,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH,\n",
    "    shuffle=False   # keep order stable for metrics/plots\n",
    ")\n",
    "\n",
    "# Class names (e.g., ['01-minor','02-moderate','03-severe'])\n",
    "class_names = train_ds.class_names\n",
    "NUM_CLASSES = len(class_names)\n",
    "print(\"Classes:\", class_names)\n",
    "\n",
    "# (Optional) pretty labels for display, stripping the numeric prefixes\n",
    "pretty_names = [c.split('-', 1)[-1] if '-' in c else c for c in class_names]\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "def configure(ds, training=False):\n",
    "    ds = ds.cache()\n",
    "    if training:\n",
    "        ds = ds.shuffle(1024, seed=SEED)\n",
    "    return ds.prefetch(AUTOTUNE)\n",
    "\n",
    "train_ds = configure(train_ds, training=True)\n",
    "val_ds   = configure(val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb5fff7",
   "metadata": {},
   "source": [
    "Setting up F1 function for performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f93b253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "class MacroF1Callback(keras.callbacks.Callback):\n",
    "    def __init__(self, val_ds, num_classes):\n",
    "        super().__init__()\n",
    "        self.val_ds = val_ds\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        for xb, yb in self.val_ds:\n",
    "            probs = self.model.predict(xb, verbose=0)\n",
    "            y_pred.extend(np.argmax(probs, axis=1))\n",
    "            # yb may be ints or one-hot depending on your pipeline:\n",
    "            if yb.ndim == 2:  # one-hot\n",
    "                y_true.extend(np.argmax(yb.numpy(), axis=1))\n",
    "            else:             # integer labels\n",
    "                y_true.extend(yb.numpy())\n",
    "        y_true = np.array(y_true)\n",
    "        y_pred = np.array(y_pred)\n",
    "        macro_f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "        logs = logs or {}\n",
    "        logs[\"val_macro_f1\"] = macro_f1\n",
    "        print(f\"\\nval_macro_f1: {macro_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fee2cc7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/efficientnet_v2/efficientnetv2-b0_notop.h5\n",
      "\u001b[1m24274472/24274472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ augment (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ efficientnetv2-b0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,919,312</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,843</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ augment (\u001b[38;5;33mSequential\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ efficientnetv2-b0 (\u001b[38;5;33mFunctional\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m5,919,312\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │         \u001b[38;5;34m3,843\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,923,155</span> (22.60 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,923,155\u001b[0m (22.60 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,843</span> (15.01 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,843\u001b[0m (15.01 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,919,312</span> (22.58 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m5,919,312\u001b[0m (22.58 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_augmentation = keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.08),\n",
    "    layers.RandomZoom(0.1),\n",
    "    layers.RandomContrast(0.1),\n",
    "], name=\"augment\")\n",
    "\n",
    "preprocess = keras.applications.efficientnet_v2.preprocess_input\n",
    "\n",
    "base = keras.applications.EfficientNetV2B0(\n",
    "    include_top=False, weights=\"imagenet\", input_shape=(*IMG_SIZE, 3)\n",
    ")\n",
    "base.trainable = False\n",
    "\n",
    "inputs = keras.Input(shape=(*IMG_SIZE, 3))\n",
    "x = data_augmentation(inputs)\n",
    "x = preprocess(x)\n",
    "x = base(x, training=False)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dropout(0.25)(x)\n",
    "outputs = layers.Dense(NUM_CLASSES, activation=\"softmax\")(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba15a4e2",
   "metadata": {},
   "source": [
    "# 5. Training with Checkpoint\n",
    "\n",
    "Early stopping with checkpoint. we start with frozen and then unlock the model. GPU enabled runtime will run much faster. If you plot the curves what can it tell you on the epochs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e3c412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "\n",
      "val_macro_f1: 0.4427\n",
      "\n",
      "Epoch 1: val_macro_f1 improved from None to 0.44269, saving model to best_frozen.keras\n",
      "43/43 - 71s - 2s/step - acc: 0.3930 - loss: 1.1053 - val_acc: 0.4597 - val_loss: 1.0086 - val_macro_f1: 0.4427 - learning_rate: 3.0000e-04\n",
      "Epoch 2/8\n",
      "\n",
      "val_macro_f1: 0.5865\n",
      "\n",
      "Epoch 2: val_macro_f1 improved from 0.44269 to 0.58647, saving model to best_frozen.keras\n",
      "43/43 - 55s - 1s/step - acc: 0.5139 - loss: 0.9849 - val_acc: 0.5927 - val_loss: 0.9045 - val_macro_f1: 0.5865 - learning_rate: 3.0000e-04\n",
      "Epoch 3/8\n",
      "\n",
      "val_macro_f1: 0.6340\n",
      "\n",
      "Epoch 3: val_macro_f1 improved from 0.58647 to 0.63400, saving model to best_frozen.keras\n",
      "43/43 - 46s - 1s/step - acc: 0.5403 - loss: 0.9484 - val_acc: 0.6371 - val_loss: 0.8617 - val_macro_f1: 0.6340 - learning_rate: 3.0000e-04\n",
      "Epoch 4/8\n",
      "\n",
      "val_macro_f1: 0.6479\n",
      "\n",
      "Epoch 4: val_macro_f1 improved from 0.63400 to 0.64794, saving model to best_frozen.keras\n",
      "43/43 - 39s - 916ms/step - acc: 0.5491 - loss: 0.9178 - val_acc: 0.6532 - val_loss: 0.8297 - val_macro_f1: 0.6479 - learning_rate: 3.0000e-04\n",
      "Epoch 5/8\n",
      "\n",
      "val_macro_f1: 0.6727\n",
      "\n",
      "Epoch 5: val_macro_f1 improved from 0.64794 to 0.67273, saving model to best_frozen.keras\n",
      "43/43 - 39s - 914ms/step - acc: 0.5880 - loss: 0.8932 - val_acc: 0.6774 - val_loss: 0.8010 - val_macro_f1: 0.6727 - learning_rate: 3.0000e-04\n",
      "Epoch 6/8\n",
      "\n",
      "val_macro_f1: 0.6709\n",
      "\n",
      "Epoch 6: val_macro_f1 did not improve from 0.67273\n",
      "43/43 - 41s - 955ms/step - acc: 0.6078 - loss: 0.8570 - val_acc: 0.6855 - val_loss: 0.7868 - val_macro_f1: 0.6709 - learning_rate: 3.0000e-04\n",
      "Epoch 7/8\n",
      "\n",
      "val_macro_f1: 0.6792\n",
      "\n",
      "Epoch 7: val_macro_f1 improved from 0.67273 to 0.67915, saving model to best_frozen.keras\n",
      "43/43 - 44s - 1s/step - acc: 0.6085 - loss: 0.8606 - val_acc: 0.6815 - val_loss: 0.7718 - val_macro_f1: 0.6792 - learning_rate: 3.0000e-04\n",
      "Epoch 8/8\n",
      "\n",
      "val_macro_f1: 0.6935\n",
      "\n",
      "Epoch 8: val_macro_f1 improved from 0.67915 to 0.69353, saving model to best_frozen.keras\n",
      "43/43 - 46s - 1s/step - acc: 0.6180 - loss: 0.8574 - val_acc: 0.6935 - val_loss: 0.7708 - val_macro_f1: 0.6935 - learning_rate: 3.0000e-04\n"
     ]
    }
   ],
   "source": [
    "EPOCHS_FROZEN = 8\n",
    "EPOCHS_FINE   = 12\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(3e-4),\n",
    "    loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.05),\n",
    "    metrics=[keras.metrics.CategoricalAccuracy(name=\"acc\")]\n",
    ")\n",
    "\n",
    "# If your datasets are integer labels, map to one-hot for this loss:\n",
    "train_oh = train_ds.map(lambda x, y: (x, tf.one_hot(y, NUM_CLASSES)))\n",
    "val_oh   = val_ds.map(lambda x, y: (x, tf.one_hot(y, NUM_CLASSES)))\n",
    "\n",
    "f1_cb = MacroF1Callback(val_ds, NUM_CLASSES)\n",
    "\n",
    "#Model filename\n",
    "filename = MODEL_DIR + \"\\\\best_frozen.keras\"\n",
    "\n",
    "ckpt1 = keras.callbacks.ModelCheckpoint(\n",
    "    filename, monitor=\"val_macro_f1\", mode=\"max\",\n",
    "    save_best_only=True, verbose=1\n",
    ")\n",
    "early = keras.callbacks.EarlyStopping(monitor=\"val_macro_f1\", mode=\"max\",\n",
    "                                      patience=5, restore_best_weights=True)\n",
    "reduce = keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",\n",
    "                                           factor=0.5, patience=2, verbose=1)\n",
    "\n",
    "hist1 = model.fit(\n",
    "    train_oh, validation_data=val_oh,\n",
    "    epochs=EPOCHS_FROZEN, callbacks=[f1_cb, ckpt1, early, reduce], verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d3ad1a",
   "metadata": {},
   "source": [
    "6. Unfreeze (Fine Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c354207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "\n",
      "val_macro_f1: 0.6959\n",
      "\n",
      "Epoch 1: val_macro_f1 improved from None to 0.69587, saving model to best_finetuned.keras\n",
      "43/43 - 55s - 1s/step - acc: 0.6188 - loss: 0.8328 - val_acc: 0.6976 - val_loss: 0.7406 - val_macro_f1: 0.6959 - learning_rate: 1.0000e-05\n",
      "Epoch 2/12\n",
      "\n",
      "val_macro_f1: 0.7003\n",
      "\n",
      "Epoch 2: val_macro_f1 improved from 0.69587 to 0.70025, saving model to best_finetuned.keras\n",
      "43/43 - 54s - 1s/step - acc: 0.6298 - loss: 0.8306 - val_acc: 0.7016 - val_loss: 0.7315 - val_macro_f1: 0.7003 - learning_rate: 1.0000e-05\n",
      "Epoch 3/12\n",
      "\n",
      "val_macro_f1: 0.7091\n",
      "\n",
      "Epoch 3: val_macro_f1 improved from 0.70025 to 0.70908, saving model to best_finetuned.keras\n",
      "43/43 - 52s - 1s/step - acc: 0.6554 - loss: 0.8051 - val_acc: 0.7097 - val_loss: 0.7237 - val_macro_f1: 0.7091 - learning_rate: 1.0000e-05\n",
      "Epoch 4/12\n",
      "\n",
      "val_macro_f1: 0.7206\n",
      "\n",
      "Epoch 4: val_macro_f1 improved from 0.70908 to 0.72061, saving model to best_finetuned.keras\n",
      "43/43 - 55s - 1s/step - acc: 0.6635 - loss: 0.8058 - val_acc: 0.7218 - val_loss: 0.7144 - val_macro_f1: 0.7206 - learning_rate: 1.0000e-05\n",
      "Epoch 5/12\n",
      "\n",
      "val_macro_f1: 0.7374\n",
      "\n",
      "Epoch 5: val_macro_f1 improved from 0.72061 to 0.73744, saving model to best_finetuned.keras\n",
      "43/43 - 53s - 1s/step - acc: 0.6716 - loss: 0.7973 - val_acc: 0.7379 - val_loss: 0.7087 - val_macro_f1: 0.7374 - learning_rate: 1.0000e-05\n",
      "Epoch 6/12\n",
      "\n",
      "val_macro_f1: 0.7236\n",
      "\n",
      "Epoch 6: val_macro_f1 did not improve from 0.73744\n",
      "43/43 - 51s - 1s/step - acc: 0.6525 - loss: 0.7986 - val_acc: 0.7258 - val_loss: 0.7042 - val_macro_f1: 0.7236 - learning_rate: 1.0000e-05\n",
      "Epoch 7/12\n",
      "\n",
      "val_macro_f1: 0.7309\n",
      "\n",
      "Epoch 7: val_macro_f1 did not improve from 0.73744\n",
      "43/43 - 50s - 1s/step - acc: 0.6606 - loss: 0.7942 - val_acc: 0.7339 - val_loss: 0.6994 - val_macro_f1: 0.7309 - learning_rate: 1.0000e-05\n",
      "Epoch 8/12\n",
      "\n",
      "val_macro_f1: 0.7230\n",
      "\n",
      "Epoch 8: val_macro_f1 did not improve from 0.73744\n",
      "43/43 - 51s - 1s/step - acc: 0.6576 - loss: 0.7868 - val_acc: 0.7258 - val_loss: 0.6968 - val_macro_f1: 0.7230 - learning_rate: 1.0000e-05\n",
      "Epoch 9/12\n",
      "\n",
      "val_macro_f1: 0.7092\n",
      "\n",
      "Epoch 9: val_macro_f1 did not improve from 0.73744\n",
      "43/43 - 49s - 1s/step - acc: 0.6650 - loss: 0.7781 - val_acc: 0.7137 - val_loss: 0.6975 - val_macro_f1: 0.7092 - learning_rate: 1.0000e-05\n",
      "Epoch 10/12\n",
      "\n",
      "val_macro_f1: 0.7176\n",
      "\n",
      "Epoch 10: val_macro_f1 did not improve from 0.73744\n",
      "43/43 - 51s - 1s/step - acc: 0.6672 - loss: 0.7781 - val_acc: 0.7218 - val_loss: 0.6894 - val_macro_f1: 0.7176 - learning_rate: 1.0000e-05\n"
     ]
    }
   ],
   "source": [
    "# Unfreeze top layers except BatchNorm\n",
    "for layer in base.layers[-60:]:\n",
    "    if not isinstance(layer, layers.BatchNormalization):\n",
    "        layer.trainable = True\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-5),\n",
    "    loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.05),\n",
    "    metrics=[keras.metrics.CategoricalAccuracy(name=\"acc\")]\n",
    ")\n",
    "\n",
    "filename = MODEL_DIR + \"\\\\best_finetuned.keras\"\n",
    "f1_cb2 = MacroF1Callback(val_ds, NUM_CLASSES)\n",
    "ckpt2 = keras.callbacks.ModelCheckpoint(\n",
    "    filename, monitor=\"val_macro_f1\", mode=\"max\",\n",
    "    save_best_only=True, verbose=1\n",
    ")\n",
    "\n",
    "hist2 = model.fit(\n",
    "    train_oh, validation_data=val_oh,\n",
    "    epochs=EPOCHS_FINE, callbacks=[f1_cb2, ckpt2, early, reduce], verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf4cd8a",
   "metadata": {},
   "source": [
    "7. Performance Evaluation\n",
    "there are also other things to show. can you think of any?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4c7980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 11 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000218A744FBA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Confusion matrix:\n",
      " [[69 12  1]\n",
      " [13 51 11]\n",
      " [ 2 26 63]]\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    01-minor       0.82      0.84      0.83        82\n",
      " 02-moderate       0.57      0.68      0.62        75\n",
      "   03-severe       0.84      0.69      0.76        91\n",
      "\n",
      "    accuracy                           0.74       248\n",
      "   macro avg       0.74      0.74      0.74       248\n",
      "weighted avg       0.75      0.74      0.74       248\n",
      "\n",
      "Macro-F1: 0.7374375550984426\n",
      "Found 3124 files belonging to 4 classes.\n",
      "Using 312 files for validation.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 51\u001b[39m\n\u001b[32m     49\u001b[39m ax = plt.subplot(\u001b[32m3\u001b[39m, \u001b[32m4\u001b[39m, i + \u001b[32m1\u001b[39m)\n\u001b[32m     50\u001b[39m img = images[idx].numpy().astype(\u001b[33m\"\u001b[39m\u001b[33muint8\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m true_label = \u001b[43mclass_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrue_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     52\u001b[39m pred_label = class_names[pred_ids[idx]]\n\u001b[32m     53\u001b[39m color = \u001b[33m\"\u001b[39m\u001b[33mgreen\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pred_label == true_label \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mred\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x900 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "MODEL_FILE = MODEL_DIR + \"\\\\best_finetuned.keras\"\n",
    "\n",
    "best = keras.models.load_model(MODEL_FILE, compile=False)\n",
    "best.compile(optimizer=keras.optimizers.Adam(1e-5),\n",
    "             loss=keras.losses.CategoricalCrossentropy(),\n",
    "             metrics=[\"accuracy\"])\n",
    "\n",
    "y_true, y_pred = [], []\n",
    "for xb, yb in val_ds:\n",
    "    probs = best.predict(xb, verbose=0)\n",
    "    y_pred.extend(np.argmax(probs, axis=1))\n",
    "    if yb.ndim == 2: y_true.extend(np.argmax(yb.numpy(), axis=1))\n",
    "    else:            y_true.extend(yb.numpy())\n",
    "\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "print(\"Macro-F1:\", f1_score(y_true, y_pred, average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed39da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "MODEL_FILE = MODEL_DIR + \"\\\\best_finetuned.keras\"\n",
    "\n",
    "# Create a dataset with raw images and labels (no one-hot)\n",
    "val_raw = tf.keras.utils.image_dataset_from_directory(\n",
    "    ROOT_DIR,\n",
    "    seed=SEED,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH,\n",
    "    validation_split=0.1,\n",
    "    subset=\"validation\",\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Get a small batch\n",
    "images, labels = next(iter(val_raw))\n",
    "pred_probs = best.predict(images)\n",
    "pred_ids = np.argmax(pred_probs, axis=1)\n",
    "true_ids = labels.numpy()\n",
    "\n",
    "# Plot 12 random samples\n",
    "plt.figure(figsize=(12, 9))\n",
    "for i, idx in enumerate(random.sample(range(len(images)), 12)):\n",
    "    ax = plt.subplot(3, 4, i + 1)\n",
    "    img = images[idx].numpy().astype(\"uint8\")\n",
    "    true_label = class_names[true_ids[idx]]\n",
    "    pred_label = class_names[pred_ids[idx]]\n",
    "    color = \"green\" if pred_label == true_label else \"red\"\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Pred: {pred_label}\\nTrue: {true_label}\", color=color, fontsize=9)\n",
    "    plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
