{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6de3ba25",
   "metadata": {},
   "source": [
    "Importing the required libraries and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "334537ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pathlib, json, numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "SEED = 42\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH = 32\n",
    "\n",
    "ROOT_DIR = pathlib.Path(\"data/\")\n",
    "TRAIN_DIR = ROOT_DIR / \"training\"\n",
    "VAL_DIR   = ROOT_DIR / \"validation\"\n",
    "TEST_DIR  = ROOT_DIR / \"testing\"\n",
    "\n",
    "TRAIN_SPLIT = 0.8\n",
    "VAL_SPLIT   = 0.1\n",
    "TEST_SPLIT  = 0.1\n",
    "\n",
    "MODEL_DIR = \"models\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5965bf2e",
   "metadata": {},
   "source": [
    "Distribute the images to create training, validation and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc2762b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# from collections import defaultdict\n",
    "\n",
    "# np.random.seed(SEED )\n",
    "# # Gather all images by class from all splits\n",
    "# all_images = defaultdict(list)\n",
    "# for split_dir in [TRAIN_DIR, VAL_DIR, TEST_DIR]:\n",
    "#     for class_dir in split_dir.iterdir():\n",
    "#         if class_dir.is_dir():\n",
    "#             for img_file in class_dir.glob(\"*.jpg\"):\n",
    "#                 all_images[class_dir.name].append(img_file)\n",
    "#             for img_file in class_dir.glob(\"*.png\"):\n",
    "#                 all_images[class_dir.name].append(img_file)\n",
    "#             for img_file in class_dir.glob(\"*.jpeg\"):\n",
    "#                 all_images[class_dir.name].append(img_file)\n",
    "\n",
    "# # Redistribute according to split ratios\n",
    "# for class_name, img_files in all_images.items():\n",
    "#     img_files = np.array(img_files)\n",
    "#     np.random.shuffle(img_files)\n",
    "#     n_total = len(img_files)\n",
    "#     n_train = int(n_total * TRAIN_SPLIT)\n",
    "#     n_val = int(n_total * VAL_SPLIT)\n",
    "#     n_test = n_total - n_train - n_val\n",
    "\n",
    "#     splits = [\n",
    "#         (TRAIN_DIR / class_name, img_files[:n_train]),\n",
    "#         (VAL_DIR / class_name, img_files[n_train:n_train+n_val]),\n",
    "#         (TEST_DIR / class_name, img_files[n_train+n_val:]),\n",
    "#     ]\n",
    "\n",
    "#     for target_dir, files in splits:\n",
    "#         target_dir.mkdir(parents=True, exist_ok=True)\n",
    "#         for f in files:\n",
    "#             dest = target_dir / f.name\n",
    "#             if f.resolve() != dest.resolve():\n",
    "#                 shutil.move(str(f), str(dest))\n",
    "\n",
    "# print(\"Data redistribution complete.\")\n",
    "# # Print out number of images in each split for each class\n",
    "# for class_name in all_images.keys():\n",
    "#     train_count = len(list((TRAIN_DIR / class_name).glob(\"*.jpg\"))) + len(list((TRAIN_DIR / class_name).glob(\"*.png\"))) + len(list((TRAIN_DIR / class_name).glob(\"*.jpeg\")))\n",
    "#     val_count = len(list((VAL_DIR / class_name).glob(\"*.jpg\"))) + len(list((VAL_DIR / class_name).glob(\"*.png\"))) + len(list((VAL_DIR / class_name).glob(\"*.jpeg\")))\n",
    "#     test_count = len(list((TEST_DIR / class_name).glob(\"*.jpg\"))) + len(list((TEST_DIR / class_name).glob(\"*.png\"))) + len(list((TEST_DIR / class_name).glob(\"*.jpeg\")))\n",
    "#     print(f\"Class '{class_name}': Training={train_count}, Validation={val_count}, Testing={test_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd156a7",
   "metadata": {},
   "source": [
    "This file is for training the various image classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2f7a647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2035 files belonging to 4 classes.\n",
      "Found 253 files belonging to 4 classes.\n",
      "Found 258 files belonging to 4 classes.\n",
      "Classes: ['00-none', '01-minor', '02-moderate', '03-severe']\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Build datasets WITHOUT validation_split\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    seed=SEED,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    VAL_DIR,\n",
    "    seed=SEED,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH,\n",
    "    shuffle=False   # keep order stable for metrics/plots\n",
    ")\n",
    "\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    TEST_DIR,\n",
    "    seed=SEED,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH,\n",
    "    shuffle=False   # keep order stable for metrics/plots\n",
    ")\n",
    "\n",
    "# Class names (e.g., ['01-minor','02-moderate','03-severe'])\n",
    "class_names = train_ds.class_names\n",
    "NUM_CLASSES = len(class_names)\n",
    "print(\"Classes:\", class_names)\n",
    "\n",
    "# (Optional) pretty labels for display, stripping the numeric prefixes\n",
    "pretty_names = [c.split('-', 1)[-1] if '-' in c else c for c in class_names]\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "def configure(ds, training=False):\n",
    "    ds = ds.cache()\n",
    "    if training:\n",
    "        ds = ds.shuffle(1024, seed=SEED)\n",
    "    return ds.prefetch(AUTOTUNE)\n",
    "\n",
    "train_ds = configure(train_ds, training=True)\n",
    "val_ds   = configure(val_ds)\n",
    "test_ds  = configure(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb5fff7",
   "metadata": {},
   "source": [
    "Setting up F1 function for performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f93b253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "class MacroF1Callback(keras.callbacks.Callback):\n",
    "    def __init__(self, val_ds, num_classes):\n",
    "        super().__init__()\n",
    "        self.val_ds = val_ds\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        for xb, yb in self.val_ds:\n",
    "            probs = self.model.predict(xb, verbose=0)\n",
    "            y_pred.extend(np.argmax(probs, axis=1))\n",
    "            # yb may be ints or one-hot depending on your pipeline:\n",
    "            if yb.ndim == 2:  # one-hot\n",
    "                y_true.extend(np.argmax(yb.numpy(), axis=1))\n",
    "            else:             # integer labels\n",
    "                y_true.extend(yb.numpy())\n",
    "        y_true = np.array(y_true)\n",
    "        y_pred = np.array(y_pred)\n",
    "        macro_f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "        logs = logs or {}\n",
    "        logs[\"val_macro_f1\"] = macro_f1\n",
    "        print(f\"\\nval_macro_f1: {macro_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fee2cc7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ augment (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ true_divide (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TrueDivide</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ subtract (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Subtract</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ mobilenetv2_1.00_224            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,124</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ augment (\u001b[38;5;33mSequential\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ true_divide (\u001b[38;5;33mTrueDivide\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ subtract (\u001b[38;5;33mSubtract\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ mobilenetv2_1.00_224            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m2,257,984\u001b[0m │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │         \u001b[38;5;34m5,124\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,263,108</span> (8.63 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,263,108\u001b[0m (8.63 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,124</span> (20.02 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,124\u001b[0m (20.02 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_augmentation = keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.08),\n",
    "    layers.RandomZoom(0.1),\n",
    "    layers.RandomContrast(0.1),\n",
    "], name=\"augment\")\n",
    "\n",
    "preprocess = keras.applications.mobilenet_v2.preprocess_input\n",
    "\n",
    "base = keras.applications.mobilenet_v2.MobileNetV2(\n",
    "    include_top=False, weights=\"imagenet\", input_shape=(*IMG_SIZE, 3)\n",
    ")\n",
    "base.trainable = False\n",
    "\n",
    "inputs = keras.Input(shape=(*IMG_SIZE, 3))\n",
    "x = data_augmentation(inputs)\n",
    "x = preprocess(x)\n",
    "x = base(x, training=False)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dropout(0.25)(x)\n",
    "outputs = layers.Dense(NUM_CLASSES, activation=\"softmax\")(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba15a4e2",
   "metadata": {},
   "source": [
    "# 5. Training with Checkpoint\n",
    "\n",
    "Early stopping with checkpoint. we start with frozen and then unlock the model. GPU enabled runtime will run much faster. If you plot the curves what can it tell you on the epochs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31e3c412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "val_macro_f1: 0.4978\n",
      "\n",
      "Epoch 1: val_macro_f1 improved from None to 0.49778, saving model to models\\mobilenetv2_frozen.keras\n",
      "64/64 - 32s - 495ms/step - acc: 0.3278 - loss: 1.5224 - val_acc: 0.5257 - val_loss: 1.2120 - val_macro_f1: 0.4978 - learning_rate: 3.0000e-04\n",
      "Epoch 2/30\n",
      "\n",
      "val_macro_f1: 0.5921\n",
      "\n",
      "Epoch 2: val_macro_f1 improved from 0.49778 to 0.59206, saving model to models\\mobilenetv2_frozen.keras\n",
      "64/64 - 19s - 295ms/step - acc: 0.4855 - loss: 1.2380 - val_acc: 0.6047 - val_loss: 1.0832 - val_macro_f1: 0.5921 - learning_rate: 3.0000e-04\n",
      "Epoch 3/30\n",
      "\n",
      "val_macro_f1: 0.6030\n",
      "\n",
      "Epoch 3: val_macro_f1 improved from 0.59206 to 0.60301, saving model to models\\mobilenetv2_frozen.keras\n",
      "64/64 - 19s - 298ms/step - acc: 0.5233 - loss: 1.1366 - val_acc: 0.6166 - val_loss: 0.9939 - val_macro_f1: 0.6030 - learning_rate: 3.0000e-04\n",
      "Epoch 4/30\n",
      "\n",
      "val_macro_f1: 0.6263\n",
      "\n",
      "Epoch 4: val_macro_f1 improved from 0.60301 to 0.62626, saving model to models\\mobilenetv2_frozen.keras\n",
      "64/64 - 19s - 300ms/step - acc: 0.5710 - loss: 1.0442 - val_acc: 0.6403 - val_loss: 0.9724 - val_macro_f1: 0.6263 - learning_rate: 3.0000e-04\n",
      "Epoch 5/30\n",
      "\n",
      "val_macro_f1: 0.6320\n",
      "\n",
      "Epoch 5: val_macro_f1 improved from 0.62626 to 0.63201, saving model to models\\mobilenetv2_frozen.keras\n",
      "64/64 - 19s - 293ms/step - acc: 0.5867 - loss: 1.0366 - val_acc: 0.6482 - val_loss: 0.9441 - val_macro_f1: 0.6320 - learning_rate: 3.0000e-04\n",
      "Epoch 6/30\n",
      "\n",
      "val_macro_f1: 0.6309\n",
      "\n",
      "Epoch 6: val_macro_f1 did not improve from 0.63201\n",
      "64/64 - 18s - 287ms/step - acc: 0.5995 - loss: 1.0085 - val_acc: 0.6522 - val_loss: 0.9343 - val_macro_f1: 0.6309 - learning_rate: 3.0000e-04\n",
      "Epoch 7/30\n",
      "\n",
      "val_macro_f1: 0.6456\n",
      "\n",
      "Epoch 7: val_macro_f1 improved from 0.63201 to 0.64562, saving model to models\\mobilenetv2_frozen.keras\n",
      "64/64 - 19s - 297ms/step - acc: 0.6069 - loss: 0.9926 - val_acc: 0.6601 - val_loss: 0.9206 - val_macro_f1: 0.6456 - learning_rate: 3.0000e-04\n",
      "Epoch 8/30\n",
      "\n",
      "val_macro_f1: 0.6478\n",
      "\n",
      "Epoch 8: val_macro_f1 improved from 0.64562 to 0.64776, saving model to models\\mobilenetv2_frozen.keras\n",
      "64/64 - 19s - 293ms/step - acc: 0.6270 - loss: 0.9480 - val_acc: 0.6522 - val_loss: 0.9093 - val_macro_f1: 0.6478 - learning_rate: 3.0000e-04\n",
      "Epoch 9/30\n",
      "\n",
      "val_macro_f1: 0.6756\n",
      "\n",
      "Epoch 9: val_macro_f1 improved from 0.64776 to 0.67556, saving model to models\\mobilenetv2_frozen.keras\n",
      "64/64 - 19s - 291ms/step - acc: 0.6275 - loss: 0.9414 - val_acc: 0.6838 - val_loss: 0.9043 - val_macro_f1: 0.6756 - learning_rate: 3.0000e-04\n",
      "Epoch 10/30\n",
      "\n",
      "val_macro_f1: 0.6302\n",
      "\n",
      "Epoch 10: val_macro_f1 did not improve from 0.67556\n",
      "64/64 - 19s - 290ms/step - acc: 0.6364 - loss: 0.9318 - val_acc: 0.6482 - val_loss: 0.9094 - val_macro_f1: 0.6302 - learning_rate: 3.0000e-04\n",
      "Epoch 11/30\n",
      "\n",
      "val_macro_f1: 0.6490\n",
      "\n",
      "Epoch 11: val_macro_f1 did not improve from 0.67556\n",
      "64/64 - 18s - 286ms/step - acc: 0.6482 - loss: 0.9159 - val_acc: 0.6561 - val_loss: 0.8938 - val_macro_f1: 0.6490 - learning_rate: 3.0000e-04\n",
      "Epoch 12/30\n",
      "\n",
      "val_macro_f1: 0.6632\n",
      "\n",
      "Epoch 12: val_macro_f1 did not improve from 0.67556\n",
      "64/64 - 18s - 287ms/step - acc: 0.6447 - loss: 0.8988 - val_acc: 0.6680 - val_loss: 0.8792 - val_macro_f1: 0.6632 - learning_rate: 3.0000e-04\n",
      "Epoch 13/30\n",
      "\n",
      "val_macro_f1: 0.6511\n",
      "\n",
      "Epoch 13: val_macro_f1 did not improve from 0.67556\n",
      "64/64 - 19s - 292ms/step - acc: 0.6595 - loss: 0.8798 - val_acc: 0.6561 - val_loss: 0.8836 - val_macro_f1: 0.6511 - learning_rate: 3.0000e-04\n",
      "Epoch 14/30\n",
      "\n",
      "val_macro_f1: 0.6846\n",
      "\n",
      "Epoch 14: val_macro_f1 improved from 0.67556 to 0.68459, saving model to models\\mobilenetv2_frozen.keras\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0001500000071246177.\n",
      "64/64 - 19s - 294ms/step - acc: 0.6722 - loss: 0.8627 - val_acc: 0.6996 - val_loss: 0.8872 - val_macro_f1: 0.6846 - learning_rate: 3.0000e-04\n",
      "Epoch 15/30\n",
      "\n",
      "val_macro_f1: 0.6785\n",
      "\n",
      "Epoch 15: val_macro_f1 did not improve from 0.68459\n",
      "64/64 - 18s - 288ms/step - acc: 0.6742 - loss: 0.8714 - val_acc: 0.6798 - val_loss: 0.8788 - val_macro_f1: 0.6785 - learning_rate: 1.5000e-04\n",
      "Epoch 16/30\n",
      "\n",
      "val_macro_f1: 0.6676\n",
      "\n",
      "Epoch 16: val_macro_f1 did not improve from 0.68459\n",
      "64/64 - 19s - 289ms/step - acc: 0.6830 - loss: 0.8531 - val_acc: 0.6719 - val_loss: 0.8733 - val_macro_f1: 0.6676 - learning_rate: 1.5000e-04\n",
      "Epoch 17/30\n",
      "\n",
      "val_macro_f1: 0.6778\n",
      "\n",
      "Epoch 17: val_macro_f1 did not improve from 0.68459\n",
      "64/64 - 19s - 295ms/step - acc: 0.6678 - loss: 0.8776 - val_acc: 0.6798 - val_loss: 0.8728 - val_macro_f1: 0.6778 - learning_rate: 1.5000e-04\n",
      "Epoch 18/30\n",
      "\n",
      "val_macro_f1: 0.6585\n",
      "\n",
      "Epoch 18: val_macro_f1 did not improve from 0.68459\n",
      "64/64 - 19s - 300ms/step - acc: 0.6757 - loss: 0.8559 - val_acc: 0.6640 - val_loss: 0.8733 - val_macro_f1: 0.6585 - learning_rate: 1.5000e-04\n",
      "Epoch 19/30\n",
      "\n",
      "val_macro_f1: 0.6589\n",
      "\n",
      "Epoch 19: val_macro_f1 did not improve from 0.68459\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 7.500000356230885e-05.\n",
      "64/64 - 21s - 321ms/step - acc: 0.6545 - loss: 0.8722 - val_acc: 0.6640 - val_loss: 0.8738 - val_macro_f1: 0.6589 - learning_rate: 1.5000e-04\n"
     ]
    }
   ],
   "source": [
    "EPOCHS_FROZEN = 30\n",
    "EPOCHS_FINE   = 60\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(3e-4),\n",
    "    loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.05),\n",
    "    metrics=[keras.metrics.CategoricalAccuracy(name=\"acc\")]\n",
    ")\n",
    "\n",
    "# If your datasets are integer labels, map to one-hot for this loss:\n",
    "train_oh = train_ds.map(lambda x, y: (x, tf.one_hot(y, NUM_CLASSES)))\n",
    "val_oh   = val_ds.map(lambda x, y: (x, tf.one_hot(y, NUM_CLASSES)))\n",
    "\n",
    "f1_cb = MacroF1Callback(val_ds, NUM_CLASSES)\n",
    "\n",
    "#Model filename\n",
    "filename = MODEL_DIR + \"\\\\mobilenetv2_frozen.keras\"\n",
    "\n",
    "ckpt1 = keras.callbacks.ModelCheckpoint(\n",
    "    filename, monitor=\"val_macro_f1\", mode=\"max\",\n",
    "    save_best_only=True, verbose=1\n",
    ")\n",
    "early = keras.callbacks.EarlyStopping(monitor=\"val_macro_f1\", mode=\"max\",\n",
    "                                      patience=5, restore_best_weights=True)\n",
    "reduce = keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",\n",
    "                                           factor=0.5, patience=2, verbose=1)\n",
    "\n",
    "hist1 = model.fit(\n",
    "    train_oh, validation_data=val_oh,\n",
    "    epochs=EPOCHS_FROZEN, callbacks=[f1_cb, ckpt1, early, reduce], verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d3ad1a",
   "metadata": {},
   "source": [
    "6. Unfreeze (Fine Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c354207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "\n",
      "val_macro_f1: 0.6645\n",
      "\n",
      "Epoch 1: val_macro_f1 improved from None to 0.66449, saving model to models\\mobilenetv2_finetuned.keras\n",
      "64/64 - 36s - 564ms/step - acc: 0.6467 - loss: 0.8839 - val_acc: 0.6680 - val_loss: 0.8809 - val_macro_f1: 0.6645 - learning_rate: 1.0000e-05\n",
      "Epoch 2/60\n",
      "\n",
      "val_macro_f1: 0.6793\n",
      "\n",
      "Epoch 2: val_macro_f1 improved from 0.66449 to 0.67931, saving model to models\\mobilenetv2_finetuned.keras\n",
      "64/64 - 23s - 362ms/step - acc: 0.6830 - loss: 0.8256 - val_acc: 0.6798 - val_loss: 0.8624 - val_macro_f1: 0.6793 - learning_rate: 1.0000e-05\n",
      "Epoch 3/60\n",
      "\n",
      "val_macro_f1: 0.6846\n",
      "\n",
      "Epoch 3: val_macro_f1 improved from 0.67931 to 0.68457, saving model to models\\mobilenetv2_finetuned.keras\n",
      "64/64 - 23s - 365ms/step - acc: 0.6919 - loss: 0.8184 - val_acc: 0.6877 - val_loss: 0.8336 - val_macro_f1: 0.6846 - learning_rate: 1.0000e-05\n",
      "Epoch 4/60\n",
      "\n",
      "val_macro_f1: 0.6803\n",
      "\n",
      "Epoch 4: val_macro_f1 did not improve from 0.68457\n",
      "64/64 - 23s - 353ms/step - acc: 0.7219 - loss: 0.7803 - val_acc: 0.6877 - val_loss: 0.8296 - val_macro_f1: 0.6803 - learning_rate: 1.0000e-05\n",
      "Epoch 5/60\n",
      "\n",
      "val_macro_f1: 0.6971\n",
      "\n",
      "Epoch 5: val_macro_f1 improved from 0.68457 to 0.69713, saving model to models\\mobilenetv2_finetuned.keras\n",
      "64/64 - 23s - 362ms/step - acc: 0.7356 - loss: 0.7484 - val_acc: 0.6996 - val_loss: 0.8274 - val_macro_f1: 0.6971 - learning_rate: 1.0000e-05\n",
      "Epoch 6/60\n",
      "\n",
      "val_macro_f1: 0.6931\n",
      "\n",
      "Epoch 6: val_macro_f1 did not improve from 0.69713\n",
      "64/64 - 22s - 350ms/step - acc: 0.7391 - loss: 0.7355 - val_acc: 0.6917 - val_loss: 0.8208 - val_macro_f1: 0.6931 - learning_rate: 1.0000e-05\n",
      "Epoch 7/60\n",
      "\n",
      "val_macro_f1: 0.6976\n",
      "\n",
      "Epoch 7: val_macro_f1 improved from 0.69713 to 0.69764, saving model to models\\mobilenetv2_finetuned.keras\n",
      "64/64 - 23s - 354ms/step - acc: 0.7464 - loss: 0.7198 - val_acc: 0.6996 - val_loss: 0.8084 - val_macro_f1: 0.6976 - learning_rate: 1.0000e-05\n",
      "Epoch 8/60\n",
      "\n",
      "val_macro_f1: 0.7143\n",
      "\n",
      "Epoch 8: val_macro_f1 improved from 0.69764 to 0.71430, saving model to models\\mobilenetv2_finetuned.keras\n",
      "64/64 - 42s - 649ms/step - acc: 0.7514 - loss: 0.7024 - val_acc: 0.7154 - val_loss: 0.8153 - val_macro_f1: 0.7143 - learning_rate: 1.0000e-05\n",
      "Epoch 9/60\n",
      "\n",
      "val_macro_f1: 0.7191\n",
      "\n",
      "Epoch 9: val_macro_f1 improved from 0.71430 to 0.71908, saving model to models\\mobilenetv2_finetuned.keras\n",
      "64/64 - 23s - 358ms/step - acc: 0.7494 - loss: 0.7029 - val_acc: 0.7233 - val_loss: 0.8048 - val_macro_f1: 0.7191 - learning_rate: 1.0000e-05\n",
      "Epoch 10/60\n",
      "\n",
      "val_macro_f1: 0.7112\n",
      "\n",
      "Epoch 10: val_macro_f1 did not improve from 0.71908\n",
      "64/64 - 23s - 352ms/step - acc: 0.7838 - loss: 0.6576 - val_acc: 0.7115 - val_loss: 0.7986 - val_macro_f1: 0.7112 - learning_rate: 1.0000e-05\n",
      "Epoch 11/60\n",
      "\n",
      "val_macro_f1: 0.7205\n",
      "\n",
      "Epoch 11: val_macro_f1 improved from 0.71908 to 0.72052, saving model to models\\mobilenetv2_finetuned.keras\n",
      "64/64 - 22s - 350ms/step - acc: 0.7803 - loss: 0.6557 - val_acc: 0.7273 - val_loss: 0.7847 - val_macro_f1: 0.7205 - learning_rate: 1.0000e-05\n",
      "Epoch 12/60\n",
      "\n",
      "val_macro_f1: 0.7222\n",
      "\n",
      "Epoch 12: val_macro_f1 improved from 0.72052 to 0.72221, saving model to models\\mobilenetv2_finetuned.keras\n",
      "64/64 - 23s - 353ms/step - acc: 0.7956 - loss: 0.6515 - val_acc: 0.7194 - val_loss: 0.8019 - val_macro_f1: 0.7222 - learning_rate: 1.0000e-05\n",
      "Epoch 13/60\n",
      "\n",
      "val_macro_f1: 0.7136\n",
      "\n",
      "Epoch 13: val_macro_f1 did not improve from 0.72221\n",
      "64/64 - 40s - 628ms/step - acc: 0.7946 - loss: 0.6287 - val_acc: 0.7233 - val_loss: 0.7805 - val_macro_f1: 0.7136 - learning_rate: 1.0000e-05\n",
      "Epoch 14/60\n",
      "\n",
      "val_macro_f1: 0.7194\n",
      "\n",
      "Epoch 14: val_macro_f1 did not improve from 0.72221\n",
      "64/64 - 23s - 362ms/step - acc: 0.8128 - loss: 0.6160 - val_acc: 0.7194 - val_loss: 0.7874 - val_macro_f1: 0.7194 - learning_rate: 1.0000e-05\n",
      "Epoch 15/60\n",
      "\n",
      "val_macro_f1: 0.7078\n",
      "\n",
      "Epoch 15: val_macro_f1 did not improve from 0.72221\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
      "64/64 - 23s - 367ms/step - acc: 0.8138 - loss: 0.6150 - val_acc: 0.7036 - val_loss: 0.8166 - val_macro_f1: 0.7078 - learning_rate: 1.0000e-05\n",
      "Epoch 16/60\n",
      "\n",
      "val_macro_f1: 0.7228\n",
      "\n",
      "Epoch 16: val_macro_f1 improved from 0.72221 to 0.72280, saving model to models\\mobilenetv2_finetuned.keras\n",
      "64/64 - 25s - 386ms/step - acc: 0.8280 - loss: 0.5936 - val_acc: 0.7233 - val_loss: 0.7844 - val_macro_f1: 0.7228 - learning_rate: 5.0000e-06\n",
      "Epoch 17/60\n",
      "\n",
      "val_macro_f1: 0.7272\n",
      "\n",
      "Epoch 17: val_macro_f1 improved from 0.72280 to 0.72721, saving model to models\\mobilenetv2_finetuned.keras\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "64/64 - 23s - 366ms/step - acc: 0.8113 - loss: 0.5951 - val_acc: 0.7273 - val_loss: 0.7862 - val_macro_f1: 0.7272 - learning_rate: 5.0000e-06\n",
      "Epoch 18/60\n",
      "\n",
      "val_macro_f1: 0.7241\n",
      "\n",
      "Epoch 18: val_macro_f1 did not improve from 0.72721\n",
      "64/64 - 23s - 361ms/step - acc: 0.8319 - loss: 0.5719 - val_acc: 0.7273 - val_loss: 0.7759 - val_macro_f1: 0.7241 - learning_rate: 2.5000e-06\n",
      "Epoch 19/60\n",
      "\n",
      "val_macro_f1: 0.7294\n",
      "\n",
      "Epoch 19: val_macro_f1 improved from 0.72721 to 0.72944, saving model to models\\mobilenetv2_finetuned.keras\n",
      "64/64 - 23s - 360ms/step - acc: 0.8418 - loss: 0.5628 - val_acc: 0.7312 - val_loss: 0.7806 - val_macro_f1: 0.7294 - learning_rate: 2.5000e-06\n",
      "Epoch 20/60\n",
      "\n",
      "val_macro_f1: 0.7216\n",
      "\n",
      "Epoch 20: val_macro_f1 did not improve from 0.72944\n",
      "64/64 - 22s - 341ms/step - acc: 0.8300 - loss: 0.5775 - val_acc: 0.7233 - val_loss: 0.7747 - val_macro_f1: 0.7216 - learning_rate: 2.5000e-06\n",
      "Epoch 21/60\n",
      "\n",
      "val_macro_f1: 0.7170\n",
      "\n",
      "Epoch 21: val_macro_f1 did not improve from 0.72944\n",
      "64/64 - 41s - 643ms/step - acc: 0.8472 - loss: 0.5570 - val_acc: 0.7194 - val_loss: 0.7788 - val_macro_f1: 0.7170 - learning_rate: 2.5000e-06\n",
      "Epoch 22/60\n",
      "\n",
      "val_macro_f1: 0.7273\n",
      "\n",
      "Epoch 22: val_macro_f1 did not improve from 0.72944\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-06.\n",
      "64/64 - 22s - 338ms/step - acc: 0.8408 - loss: 0.5696 - val_acc: 0.7273 - val_loss: 0.7760 - val_macro_f1: 0.7273 - learning_rate: 2.5000e-06\n",
      "Epoch 23/60\n",
      "\n",
      "val_macro_f1: 0.7192\n",
      "\n",
      "Epoch 23: val_macro_f1 did not improve from 0.72944\n",
      "64/64 - 22s - 339ms/step - acc: 0.8428 - loss: 0.5632 - val_acc: 0.7233 - val_loss: 0.7760 - val_macro_f1: 0.7192 - learning_rate: 1.2500e-06\n",
      "Epoch 24/60\n",
      "\n",
      "val_macro_f1: 0.7217\n",
      "\n",
      "Epoch 24: val_macro_f1 did not improve from 0.72944\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "64/64 - 22s - 344ms/step - acc: 0.8388 - loss: 0.5604 - val_acc: 0.7233 - val_loss: 0.7777 - val_macro_f1: 0.7217 - learning_rate: 1.2500e-06\n"
     ]
    }
   ],
   "source": [
    "# Unfreeze top layers except BatchNorm\n",
    "for layer in base.layers[-60:]:\n",
    "    if not isinstance(layer, layers.BatchNormalization):\n",
    "        layer.trainable = True\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-5),\n",
    "    loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.05),\n",
    "    metrics=[keras.metrics.CategoricalAccuracy(name=\"acc\")]\n",
    ")\n",
    "\n",
    "filename = MODEL_DIR + \"\\\\mobilenetv2_finetuned.keras\"\n",
    "f1_cb2 = MacroF1Callback(val_ds, NUM_CLASSES)\n",
    "ckpt2 = keras.callbacks.ModelCheckpoint(\n",
    "    filename, monitor=\"val_macro_f1\", mode=\"max\",\n",
    "    save_best_only=True, verbose=1\n",
    ")\n",
    "\n",
    "hist2 = model.fit(\n",
    "    train_oh, validation_data=val_oh,\n",
    "    epochs=EPOCHS_FINE, callbacks=[f1_cb2, ckpt2, early, reduce], verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf4cd8a",
   "metadata": {},
   "source": [
    "7. Performance Evaluation\n",
    "there are also other things to show. can you think of any?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de4c7980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      " [[64  2  3  2]\n",
      " [ 2 41  7  0]\n",
      " [ 9 11 27 20]\n",
      " [ 4  4 13 49]]\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     00-none       0.81      0.90      0.85        71\n",
      "    01-minor       0.71      0.82      0.76        50\n",
      " 02-moderate       0.54      0.40      0.46        67\n",
      "   03-severe       0.69      0.70      0.70        70\n",
      "\n",
      "    accuracy                           0.70       258\n",
      "   macro avg       0.69      0.71      0.69       258\n",
      "weighted avg       0.69      0.70      0.69       258\n",
      "\n",
      "Macro-F1: 0.6922916287809905\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "MODEL_FILE = MODEL_DIR + \"\\\\mobilenetv2_finetuned.keras\"\n",
    "\n",
    "best = keras.models.load_model(MODEL_FILE, compile=False)\n",
    "best.compile(optimizer=keras.optimizers.Adam(1e-5),\n",
    "             loss=keras.losses.CategoricalCrossentropy(),\n",
    "             metrics=[\"accuracy\"])\n",
    "\n",
    "y_true, y_pred = [], []\n",
    "for xb, yb in test_ds:\n",
    "    probs = best.predict(xb, verbose=0)\n",
    "    y_pred.extend(np.argmax(probs, axis=1))\n",
    "    if yb.ndim == 2: y_true.extend(np.argmax(yb.numpy(), axis=1))\n",
    "    else:            y_true.extend(yb.numpy())\n",
    "\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "print(\"Macro-F1:\", f1_score(y_true, y_pred, average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cb1097",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ed39da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7626 files belonging to 6 classes.\n",
      "Using 762 files for validation.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'best' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Get a small batch\u001b[39;00m\n\u001b[32m     17\u001b[39m images, labels = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(val_raw))\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m pred_probs = \u001b[43mbest\u001b[49m.predict(images)\n\u001b[32m     19\u001b[39m pred_ids = np.argmax(pred_probs, axis=\u001b[32m1\u001b[39m)\n\u001b[32m     20\u001b[39m true_ids = labels.numpy()\n",
      "\u001b[31mNameError\u001b[39m: name 'best' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "MODEL_FILE = MODEL_DIR + \"\\\\mobilenetv2_finetuned.keras\"\n",
    "\n",
    "# Create a dataset with raw images and labels (no one-hot)\n",
    "val_raw = tf.keras.utils.image_dataset_from_directory(\n",
    "    ROOT_DIR,\n",
    "    seed=SEED,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH,\n",
    "    validation_split=0.1,\n",
    "    subset=\"validation\",\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Get a small batch\n",
    "images, labels = next(iter(val_raw))\n",
    "pred_probs = best.predict(images)\n",
    "pred_ids = np.argmax(pred_probs, axis=1)\n",
    "true_ids = labels.numpy()\n",
    "\n",
    "# Plot 12 random samples\n",
    "plt.figure(figsize=(12, 9))\n",
    "for i, idx in enumerate(random.sample(range(len(images)), 12)):\n",
    "    ax = plt.subplot(3, 4, i + 1)\n",
    "    img = images[idx].numpy().astype(\"uint8\")\n",
    "    true_label = class_names[true_ids[idx]]\n",
    "    pred_label = class_names[pred_ids[idx]]\n",
    "    color = \"green\" if pred_label == true_label else \"red\"\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Pred: {pred_label}\\nTrue: {true_label}\", color=color, fontsize=9)\n",
    "    plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
